\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{url}
\usepackage{multirow}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\by}{\by}
\newcommand{\bz}{\mathbf{z}}

\begin{document}

\title{Neuro-Symbolic Safety Nets: Fusing Optimized Deep Learning with Interpretable Logic for Trustworthy Diagnosis}

\author{\IEEEauthorblockN{1\textsuperscript{st} Tomasz Karpiński}
\IEEEauthorblockA{\textit{Faculty of Physics and Applied Computer Science} \\
\textit{AGH University of Krakow}\\
Krakow, Poland \\
 tkarpinski@student.agh.edu.pl}
}

\maketitle

\begin{abstract}
 The adoption of Artificial Intelligence in high-stakes medical domains is hindered by the "black box" nature of deep neural networks. While accurate, standard models often lack the interpretability required for clinical trust and robustness against edge cases. This paper addresses this gap by proposing a unified, Hybrid Neuro-Symbolic Framework for trustworthy medical diagnosis. We combine a fixed-architecture Multi-Layer Perceptron (MLP), optimized across a broad spectrum of algorithms (including Adam, Adagrad, and Lion) via rigorous Hyperparameter Optimization (HPO) techniques such as Random Search \cite{javeed2019random}, Bayesian Optimization (Optuna) \cite{hariharan2025optuna}, Genetic Algorithms \cite{lambora2019ga}, and the Enhanced ALSHADE algorithm \cite{karpinski2025deullm}, with a symbolic expert layer comprising Rule-Based Reasoning, Fuzzy Logic, and Bayesian probabilistic updates. Additionally, we evaluate robust outlier detection strategies (Isolation Forest \cite{cheng2019iforest}) and incorporate a Robust Representation Learning module employing Principal Component Analysis (PCA) and Autoencoder-based latent embeddings to mitigate noise and dimensionality. Using the CDC Diabetes Dataset as a proxy for medical diagnostics, our system achieves a peak neural ROC-AUC of 0.82, with a notable Sensitivity (Recall) of 0.60, significantly outperforming standard baselines on the minority class. While the neuro-symbolic integration introduces a trade-off in overall discrimination (ROC-AUC 0.79), it provides critical safety guarantees. We demonstrate that the symbolic layer acts as a safety audit, identifying 20.36\% of "false negative" cases that purely data-driven models miss. This work illustrates a pathway toward "Human-in-the-loop" AI systems that prioritize safety and transparency alongside statistical performance.
\end{abstract}

\begin{IEEEkeywords}
Neuro-Symbolic AI, Hyperparameter Optimization, Explainable AI, Medical Diagnosis, Trustworthy AI, Autoencoders, Evolutionary Computing, Enhanced ALSHADE
\end{IEEEkeywords}

\section{Introduction}
In the domain of healthcare, the cost of an error is not measured in latency or throughput, but in human well-being. Chronic conditions like Diabetes require early and accurate detection to prevent severe complications. While Deep Learning has revolutionized predictive capabilities in many fields, its application in medicine faces a critical "Trust Gap." A neural network may predict a 99\% probability of health, but without an explanation, a clinician cannot verify the reasoning against medical guidelines. Furthermore, purely data-driven models are often "brittle," failing silently on out-of-distribution samples or rare edge cases.

To bridge this gap, we must move beyond black-box accuracy and towards **Trustworthy AI**. This requires systems that are not only high-performing but also interpretable, robust, and aligned with expert knowledge.

This paper presents a comprehensive implementation of a **Hybrid Neuro-Symbolic Diagnostic System**. We address the conflict between accuracy and interpretability by fusing two distinct paradigms: the statistical learning power of deep neural networks and the transparent, logic-driven reasoning of classical Artificial Intelligence.

Our contributions are multi-fold:
\begin{enumerate}
    \item We evaluate \textbf{robust outlier detection mechanisms}, specifically comparing standard Z-score pruning against the Isolation Forest algorithm \cite{cheng2019iforest}, to ensure high-quality training data.
    \item We formulate a \textbf{hybrid embedding strategy} that compares raw features, PCA-reduced vectors \cite{kherif2020pca}, and non-linear Autoencoder latent spaces \cite{tschannen2018ae} to determine the optimal input manifold for medical diagnosis.
    \item We implement and rigorously optimize a baseline MLP using distinct Hyperparameter Optimization (HPO) strategies: Random Search, Bayesian Optimization (Optuna), and Evolutionary Algorithms (including Enhanced ALSHADE \cite{karpinski2025deullm}).
    \item \textbf{Optimizer Analysis:} We conduct an extensive analysis of optimization algorithms, rigorously tuning \textbf{Adam \cite{zhang2026adam}, RMSProp \cite{elshamy2023rmsprop}, SGD \cite{xing2018sgd}, Adagrad \cite{venkatesh2020adagrad}, Adadelta \cite{veeramsetty2025adadelta}, and Lion \cite{chen2023lion}} to identify convergence properties specific to medical tabular data topologies.
    \item We augment this neural baseline with a "Neuro-Symbolic Interpretability Layer" consisting of explicit medical rules \cite{turgunbaev2025rule}, fuzzy inference systems \cite{arji2019fuzzy}, and Bayesian probability updates, providing human-readable explanations for model predictions.
    \item We perform a thorough error analysis and unsupervised exploration (PCA, K-Means \cite{ahmed2020kmeans}), demonstrating how hybrid systems can expose the limitations of pure Deep Learning on complex, hard-to-diagnose patient profiles.
\end{enumerate}

The remainder of this paper is organized as follows: Section II details our methodology, including dataset preprocessing, model architecture, and the neuro-symbolic components. Section III presents our experimental results on HPO convergence and interpretability. Section IV discusses the implications for robustness and clinical adoption, and Section V concludes with future directions.

\section{Related Work}

\subsection{Deep Learning for Tabular Data}
While Convolutional Neural Networks (CNNs) dominate imaging, tabular data lacks spatial locality. Recent works have explored specialized architectures like TabNet and 1D-CNNs. However, Multi-Layer Perceptrons (MLPs) remain a strong baseline when properly regularized. We extend this by focusing on the \textit{input representation} rather than just architecture depth.

\subsection{Neuro-Symbolic AI in Medicine}
Purely connectionist models lack explainability. Neuro-symbolic approaches aim to embed logical rules into neural training or post-processing. Our work aligns with the "Symbolic Neuro-Symbolic" paradigm, where symbolic reasoning acts as a post-hoc reasoning module to validate neural outputs.

\subsection{Hyperparameter Optimization (HPO)}
Grid search is computationally prohibitive for deep networks. Bayesian Optimization (using Gaussian Processes) and Evolutionary Algorithms (Genetic Algorithms) offer efficient navigation of non-convex loss landscapes. We systematically compare these against standard baselines.

\subsection{Hybrid Neuro--Symbolic Methods}
Recent approaches have sought to combine the learning capabilities of neural networks with the reasoning power of symbolic logic. Techniques include compiling rules into network weights, regularizing loss functions with logical constraints, and hybrid architectures that route data between neural and symbolic modules. Our approach differs by using a loose coupling where the symbolic layer acts as an independent auditor and safety mechanism for the neural predictions.
\section{Methodology}

\subsection{Dataset}
 This study utilizes the \textbf{CDC Diabetes Health Indicators Dataset}, a large-scale health survey widely used as a benchmark for predictive medical modeling. The dataset serves as a proxy for complex diagnostic tasks, containing features such as Body Mass Index (BMI), Age, High Blood Pressure, Cholesterol levels, and General Health self-assessments.

Figure \ref{fig:class_dist} illustrates the significant class imbalance in the dataset, with the non-diabetic class (0) vastly outnumbering the diabetic class (1). This imbalance necessitates the use of stratified sampling and metrics like ROC-AUC over simple accuracy.

\input{generated_tables/table_dataset_stats.tex}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/class_distribution_comparison.png}}
\caption{Comparison of Class Distributions: Original vs. Z-score vs. Isolation Forest. While Z-score aggressively prunes data (often disproportionately affecting the minority class), Isolation Forest retains a higher volume of valid samples while targeting specific anomalies.}
\label{fig:class_dist}
\end{figure}

To address the significant class imbalance (approx. 1:7) and improve the sensitivity of our diagnostic model, we implemented a "Total Defense" strategy consisting of three synergistic techniques applied after initial cleaning.

\begin{enumerate}
    \item \textbf{Robust Outlier Detection (Isolation Forest):} We employ the Isolation Forest algorithm \cite{cheng2019iforest} to remove statistical anomalies from the training data. Unlike simple Z-score thresholding, which cuts off tails of distributions, Isolation Forest isolates anomalies by randomly selecting a feature and then randomly selecting a split value. This preserves valid minority class samples that might be statistically "rare" but medically valid. Figure \ref{fig:outlier_comp} demonstrates the comparative effect of outlier removal methods.

    \item \textbf{Data-Level Random Oversampling:} Applied strictly to the training set after outlier removal, we utilize Random Oversampling to duplicate samples from the minority class (Diabetes) until it matches the majority class count. This ensures the model sees enough diabetic cases to learn their characteristics, preventing bias towards the majority "healthy" class.

    \item \textbf{Algorithm-Level Weighted Loss:} We replaced the standard Binary Cross Entropy (BCE) loss with \texttt{BCEWithLogitsLoss}, incorporating a positive class weight ($pos\_weight$). The weight is calculated as $Weight = \frac{N_{negative}}{N_{positive}}$, scaling the loss for missing a diabetic case. This forces the optimizer to prioritize recall/sensitivity, penalizing false negatives significantly more than false positives.

    \item \textbf{Inference-Level Dynamic Threshold Moving:} Instead of using the default classification threshold of 0.5, we dynamically calculate the optimal threshold that maximizes the F1-Score on the validation set. This adapts the model's decision logic to the specific difficulty of the task, trading off Precision and Recall to optimize for the most balanced diagnostic performance.
\end{enumerate}

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{figures/outlier_method_comparison.png}}
\caption{Comprehensive comparison of feature distributions: Original dataset vs. Z-score cleaning vs. Isolation Forest cleaning. Isolation Forest provides a more targeted removal of anomalies while preserving the integrity of the feature space for minority class samples.}
\label{fig:outlier_comp}
\end{figure}

Figure \ref{fig:pairplot} provides a pairwise visualization of selected features, highlighting the complex, non-linear relationships and overlap between classes that motivate the use of a neural network over simpler linear models.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/pairplot.png}}
\caption{Pairplot of selected features colored by target class. The substantial overlap between classes indicates a non-trivial classification boundary.}
\label{fig:pairplot}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{figures/feature_hist.png}
\caption{Feature distributions for all input variables. The histograms reveal the diverse scales and distributions (some binary, some continuous) across the 21 features.}
\label{fig:feature_hist}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{figures/corr_heatmap.png}
\caption{Correlation heatmap of the top 10 input features.}
\label{fig:corr_heatmap}
\end{figure}

\subsection{Representation Learning}
 To ensure robust feature extraction, we investigate three input representations: Raw features, PCA projections, and Autoencoder embeddings.

\subsubsection{Principal Component Analysis (PCA)}
 We project $\bx$ onto a linear orthogonal subspace to maximize variance retention. Let $\mathbf{W}_{pca} \in \mathbb{R}^{d \times k}$ be the projection matrix corresponding to the top $k$ eigenvectors of the covariance matrix. The transformed input is $\bz_{pca} = \mathbf{W}_{pca}^T \bx$.

Figures \ref{fig:pca_labels} and \ref{fig:pca_clusters} visualize the data structure via PCA. To further evaluate our improvements, we present the data structure after Isolation Forest cleaning in Figures \ref{fig:pca_labels_iso} and \ref{fig:pca_clusters_iso}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_pca_labels.png}}
\caption{PCA Visualization colored by True Labels (Baseline Z-score).}
\label{fig:pca_labels}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_pca_clusters.png}}
\caption{PCA Visualization colored by K-Means Clusters (Baseline Z-score).}
\label{fig:pca_clusters}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_pca_labels_iso.png}}
\caption{PCA Visualization colored by True Labels (Isolation Forest).}
\label{fig:pca_labels_iso}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_pca_clusters_iso.png}}
\caption{PCA Visualization colored by K-Means Clusters (Isolation Forest).}
\label{fig:pca_clusters_iso}
\end{figure}

The PCA plots reveal significant overlap between classes, confirming the inherent difficulty of the classification task. K-Means clustering ($k=2$) captures the broad structural split but does not perfectly align with the disease labels, suggesting that "Diabetes" is not a single, geometrically distinct cluster in feature space but rather a complex boundary condition.

To better capture non-linear relationships and compare outlier detection methods, we employed Uniform Manifold Approximation and Projection (UMAP) as shown in Figure \ref{fig:umap_comp}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{figures/umap_comparison.png}}
\caption{Manifold Comparison via UMAP: Z-score vs. Isolation Forest cleaning. Isolation Forest preserves a more complex manifold structure while effectively isolating anomalous points.}
\label{fig:umap_comp}
\end{figure}

\subsubsection{Autoencoder Embeddings (AE)}
 To capture non-linear correlations, we employ an undercomplete Autoencoder. The encoder $f_\theta$ maps $\bx$ to a latent code $\bz_{ae}$, and the decoder $g_\phi$ reconstructs $\hat{\bx}$. The objective is to minimize the reconstruction loss $\mathcal{L}_{rec}$ alongside an $L_2$ regularization term:
\begin{equation}
    \mathcal{L}_{AE} = \frac{1}{N} \sum_{i=1}^N ||\bx_i - g_\phi(f_\theta(\bx_i))||^2 + \lambda ||\theta||^2
\end{equation}
The latent vector $\bz_{ae}$ serves as the compressed feature input for the downstream classification network.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/autoencoder_arch.png}}
\caption{Autoencoder Architecture. The network compresses the 21-dimensional input into a lower-dimensional latent bottleneck before reconstruction, forcing the model to learn robust feature representations.}
\label{fig:ae_arch}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_ae_labels.png}}
\caption{Autoencoder Latent Space Visualization (Baseline Z-score).}
\label{fig:ae_latent}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_ae_labels_iso.png}}
\caption{Autoencoder Latent Space Visualization (Isolation Forest).}
\label{fig:ae_latent_iso}
\end{figure}
\subsection{The Baseline Neural Network}
 We implemented a fixed-architecture Multi-Layer Perceptron (MLP) designed for binary classification (Diabetes vs. Non-Diabetes). The architecture consists of:
\begin{itemize}
    \item \textbf{Input Layer:} 21 features (after preprocessing), or the reduced dimension vector from PCA/AE.
    \item \textbf{Hidden Layers:} Two dense layers with 32 and 16 neurons respectively, using ReLU activation functions to introduce non-linearity.
    \item \textbf{Output Layer:} A single neuron with a Sigmoid activation function to output a probability score $P(y=1|x) \in [0, 1]$.
    \item \textbf{Optimization:} We investigate multiple optimizers: Adam, RMSProp, and SGD.
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/mlp_architecture.png}}
\caption{Baseline MLP Architecture. The classifier consists of two dense hidden layers with ReLU activation, followed by a sigmoid output, providing the core statistical learning capability of the system.}
\label{fig:mlp_arch}
\end{figure}

\subsection{Hyperparameter Optimization (HPO)}
 To ensure the neural network reached its maximum potential, we moved beyond manual tuning and employed three automated HPO strategies. The search space included Learning Rate ($10^{-5}$ to $10^{-2}$), Batch Size (32, 64, 128), Weight Decay ($L_2$ regularization), Dropout rate, and Adam $\beta$ parameters.

\subsubsection{Random Search}
 Used as a stochastic baseline, Random Search samples hyperparameters uniformly from the defined space. It is simple to implement but inefficient in high-dimensional spaces.

\subsubsection{Bayesian Optimization (Optuna)}
 We utilized Optuna with the Tree-structured Parzen Estimator (TPE) sampler. Unlike Random Search, Bayesian Optimization builds a probabilistic model of the objective function, allowing it to "focus" on promising regions of the hyperparameter space, theoretically converging faster to the global optimum.

\subsubsection{Evolutionary Algorithms (GA, DE, PSO, ALSHADE)}
 We implemented a suite of evolutionary strategies:
\begin{itemize}
    \item \textbf{Genetic Algorithm (GA):} A population of hyperparameter configurations evolves over generations via crossover and mutation.
    \item \textbf{Differential Evolution (DE):} Uses vector differences for perturbation.
    \item \textbf{Particle Swarm Optimization (PSO):} Simulates social behavior to traverse the search space.
    \item \textbf{Enhanced ALSHADE \cite{karpinski2025deullm}:} An adaptive variant of Differential Evolution that auto-tunes the scaling factor ($F$) and crossover rate ($CR$) during evolution, providing robust convergence without manual parameter setting.
\end{itemize}

\subsection{Neuro-Symbolic Interpretability Layer}
 To address the "black box" nature of the MLP, we developed an expert layer that operates in parallel to the neural network.

\subsubsection{Rule-Based Reasoning}
 We encoded explicit medical heuristics into IF-THEN rules. For example:
\begin{equation}
    \text{IF } (BMI \ge 30) \land (Age \ge 45) \rightarrow \text{High Risk}
\end{equation}
These rules act as "sanity checks," ensuring that high-risk profiles identified by established medical guidelines are not missed by the model.

\subsubsection{Fuzzy Logic Inference}
 Medical thresholds are rarely binary. A BMI of 29.9 is medically similar to 30.1. We modeled continuous variables like Age and BMI using Fuzzy Sets (Low, Medium, High) with triangular membership functions. We implemented \textbf{three core fuzzy inference rules} to compute a nuanced "Fuzzy Risk Score" $Score \in [0, 1]$:
\begin{itemize}
    \item \textbf{High Risk:} IF BMI is High AND Age is Old $\rightarrow$ Risk=0.8
    \item \textbf{Medium Risk:} IF BMI is Medium AND Age is Mid $\rightarrow$ Risk=0.5
    \item \textbf{Low Risk:} IF BMI is Low AND Age is Young $\rightarrow$ Risk=0.1
\end{itemize}
These rules capture the non-linear interaction between age and obesity, ensuring that marginal cases are assigned a proportional risk probability rather than a hard classification.

\subsubsection{Bayesian Probability Update}
 We implemented a standalone **Gaussian Naive Bayes (GNB)** classifier to compute an independent "second opinion" probability $p_{gnb} = P(Y=1 | \bx)$. This model is fitted purely on the training partition using the original feature space. By leveraging the conditional independence assumption, the GNB provides a statistically grounded risk estimate based on historical feature distributions, acting as a robust probabilistic baseline that is less prone to over-fitting complex non-linear patterns than the deep neural baseline.

\subsubsection{Decision Fusion}
The final diagnostic probability, $p_{hybrid}$, is computed via a consensus mechanism that integrates statistical learning with symbolic expert logic. For a given patient profile $\bx$, we first compute the unweighted average of the three probabilistic components:
\begin{equation}
    p_{consensus} = \frac{p_{mlp}(\bx) + p_{fuzzy}(\bx) + p_{gnb}(\bx)}{3}
\end{equation}
where $p_{mlp}$ is the optimized neural output, $p_{fuzzy}$ is the fuzzy risk score, and $p_{gnb}$ is the Bayesian posterior.

Finally, we apply **Rule-Based Overrides** to ensure alignment with high-confidence medical heuristics. If the explicit clinical rule set (Section III-E.1) identifies a specific high-risk profile, the system enforces a safety guarantee by setting $p_{hybrid} = \max(p_{consensus}, 0.95)$. Conversely, for low-risk clinical profiles, we set $p_{hybrid} = \min(p_{consensus}, 0.05)$. This fusion strategy ensures that statistical predictions are always bounded by established medical knowledge.

\subsubsection{Evaluation Protocol}
 To ensure the scientific validity and reproducibility of our results, we adhere to a rigorous evaluation protocol. The dataset is partitioned using a \textbf{Stratified 80/20 Train-Test split} with a fixed random seed ($42$). From the training partition, a further 10\% subset is reserved for validation to facilitate hyperparameter tuning and the calculation of optimal classification thresholds.

 Statistical significance across model configurations is evaluated using the \textbf{Wilcoxon signed-rank test}. Unlike traditional cross-validation which compares fold-level aggregates, our analysis operates on the \textbf{individual patient error vectors} within the held-out test set ($N=50,736$). We define the error for each sample as $e_i = |y_i - \hat{p}_i|$, where $y_i$ is the true binary label and $\hat{p}_i$ is the predicted probability. This approach allows us to determine if the differences in predictive behavior between models (e.g., MLP vs. Hybrid) are statistically significant at the patient level ($p < 0.05$). Friedman ranking is subsequently applied across the HPO method combinations to identify the most robust optimization strategies.

\begin{algorithm}[h]
\caption{Unified Neural--Symbolic Diagnostic Framework}
\label{alg:unified_framework}
\scriptsize
\begin{algorithmic}[1]
\Require Dataset $D$, Optimizer Set $\mathcal{O}$, HPO Methods $\mathcal{H}$,
Representation Methods $\mathcal{R}$, Hybrid Modules $\mathcal{M} = \{\text{Rules}, \text{Fuzzy}, \text{Bayesian}\}$
\Ensure Final Diagnosis Probability $p_{hybrid}$

\State \textbf{Preprocessing:}
\Statex \quad Normalize numerical features and encode categorical ones.
\Statex \quad Split data into train/val/test sets.

\State \textbf{Representation Learning:}
\For{representation method $r \in \mathcal{R}$}
    \State Compute transformed dataset $D_r$ using:
    \Statex \quad Raw features / PCA components / AE embeddings
\EndFor

\State \textbf{Neural Training with HPO:}
\For{optimizer $o \in \mathcal{O}$}
    \For{HPO strategy $h \in \mathcal{H}$}
        \State Run hyperparameter search $h$ with optimizer $o$
        \State Train MLP with best hyperparameters $(o,h)$
        \State Record metrics: Acc, Prec, Rec, F1, ROC
    \EndFor
\EndFor

\State Select best MLP model $M^*$ by Friedman ranking

\State \textbf{Hybrid Reasoning Layer:}
\State Compute MLP probability output $p_{mlp} = M^*(x)$
\State Apply fuzzy membership functions to obtain fuzzy score $p_{fuzzy}$
\State Compute Bayesian probability $p_{gnb} = P(Y=1 | \bx)$ using GNB

\State \textbf{Decision Fusion:}
\State Compute consensus: $p_{hybrid} = \frac{1}{3} (p_{mlp} + p_{fuzzy} + p_{gnb})$
\State Apply rule-based overrides for high/low risk patterns

\State \Return Final probability $p_{hybrid}$

\end{algorithmic}
\end{algorithm}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.95\linewidth]{figures/hybrid_flow.png}}
\caption{Hybrid Neuro-Symbolic Decision Flow. The system fuses neural predictions with fuzzy logic risk scores and Bayesian probability updates, governed by an expert rule set to ensure safety and interpretability.}
\label{fig:hybrid_flow}
\end{figure}

\subsection{Unsupervised Analysis}
 To understand the underlying structure of the patient data, we applied Unsupervised Learning techniques:
\begin{itemize}
    \item \textbf{PCA (Principal Component Analysis):} Reduced the 21-dimensional feature space to 2 principal components for visualization.
    \item \textbf{K-Means Clustering:} Grouped patients into latent clusters ($k=2$) to analyze if natural data groupings align with diagnostic labels.
\end{itemize}

\section{Experiments \& Results}

\subsection{Evaluation Metrics}
Since the problem is a classification task, we focus on classification metrics rather than regression metrics. The primary evaluation metrics are:
\begin{itemize}
    \item \textbf{Accuracy (Acc)}: proportion of correctly classified samples.
    \[ \text{Acc} = \frac{TP + TN}{TP + TN + FP + FN} \]
    \item \textbf{Precision (Prec)}: proportion of predicted positives that are correct.
    \[ \text{Prec} = \frac{TP}{TP + FP} \]
    \item \textbf{Recall (Rec) / Sensitivity}: proportion of actual positives correctly identified.
    \[ \text{Rec} = \frac{TP}{TP + FN} \]
    \item \textbf{F1-score (F1)}: harmonic mean of precision and recall.
    \[ F1 = 2 \cdot \frac{\text{Prec} \cdot \text{Rec}}{\text{Prec} + \text{Rec}} \]
    \item \textbf{ROC-AUC (ROC)}: Area Under the Receiver Operating Characteristic Curve, measuring the ability to distinguish between classes.
\end{itemize}

\subsection{Comparison with Baseline Classifiers}
 We compare the proposed Neuro-Symbolic approach against classical baselines (SVM, Random Forest).

\input{generated_tables/table_baselines.tex}

\subsection{HPO Performance and Convergence}
 We evaluated the efficacy of the HPO methods. Figure \ref{fig:hpo_convergence} illustrates the convergence of the best-found ROC-AUC score over sequential evaluation trials.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/hpo_convergence.png}}
\caption{HPO Convergence: Comparison of Random Search, Bayesian Optimization (Optuna), and Genetic Algorithms. Optuna demonstrates rapid convergence to the optimal region.}
\label{fig:hpo_convergence}
\end{figure}

\input{generated_tables/table_optimizers.tex}

Furthermore, to gain deeper insights into the hyperparameter search landscape, we visualize the distributions of evaluated hyperparameter combinations against their resulting ROC-AUC scores. Figure \ref{fig:hpo_distributions} presents pairwise scatter plots of key continuous hyperparameters.

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{figures/hpo_distributions.png}}
\caption{Hyperparameter Distributions vs. ROC-AUC (Baseline Z-score). Pairwise scatter plots of Learning Rate, Weight Decay, Dropout, Beta1, and Beta2, colored by the achieved ROC-AUC, reveal the impact of hyperparameter choices on model performance.}
\label{fig:hpo_distributions}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{figures/hpo_distributions_iso.png}}
\caption{Hyperparameter Distributions vs. ROC-AUC (Isolation Forest Pipeline). The landscape shows a significantly higher and more clustered performance region (ROC-AUC $> 0.82$), indicating that the "Total Defense" strategy stabilized the training process across diverse hyperparameter configurations.}
\label{fig:hpo_dist_iso}
\end{figure}

\textbf{Analysis:} As hypothesized, Bayesian Optimization (Optuna) proved the most efficient, rapidly identifying a high-performing configuration (ROC-AUC $\approx 0.81$) within fewer trials than Random Search. The Genetic Algorithm also performed well but required a larger number of evaluations to stabilize. The optimized MLP achieved a final Test ROC-AUC of \textbf{0.8212}, significantly robust for this complexity of data. Comparing Figures \ref{fig:hpo_distributions} and \ref{fig:hpo_dist_iso}, it is evident that the improved preprocessing and architectural changes not only boosted peak performance but also elevated the average performance across the search space.

\begin{table}[htbp]
\caption{Comparison of Hyperparameter Optimization Methods}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Best ROC-AUC} & \textbf{Time (s)} & \textbf{Best Epochs} \\
\hline
Random Search & 0.8286 & 176.5 & 15 \\
Bayesian (Optuna) & \textbf{0.8322} & 2143.5 & 62 \\
Genetic Algorithm & 0.8252 & 1140.5 & 32 \\
Enhanced ALSHADE & 0.8264 & 1984.2 & 50 \\
\hline
\end{tabular}
\label{tab:hpo_comparison}
\end{center}
\end{table}

\subsection{Optimization Analysis across Representations}
 We performed an extensive ablation of input representations (Raw, PCA, Autoencoder) paired with various optimizers and HPO strategies. Detailed performance tables for each representation are provided in the Supplementary Material.

\input{generated_tables/table_representation.tex}

\subsection{Baseline Performance Metrics}
\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/training_loss_convergence.png}}
\caption{Training and Validation Loss Convergence for the Best Baseline MLP. The plot illustrates the model's learning process over epochs, showing both training and validation loss curves.}
\label{fig:training_loss_convergence}
\end{figure}

 Before delving into HPO and advanced interpretability, we assess the baseline MLP's performance on the test set. Figure \ref{fig:confusion_matrix} presents the confusion matrix, which provides a detailed breakdown of correct and incorrect classifications.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{figures/confusion_matrix.png}}
\caption{Confusion Matrix: Optimized MLP (PCA + Lion). The matrix reveals the improved balance between True Positives and True Negatives achieved via dynamic thresholding.}
\label{fig:confusion_matrix}
\end{figure}

The matrix reveals a high number of True Negatives (correctly identified non-diabetic cases) but also a significant number of False Negatives (diabetic cases incorrectly classified as non-diabetic), reflecting the challenge of detecting the minority class and the imbalanced nature of the dataset.

\subsection{Diagnostic Calibration and Borderline Analysis}
A critical requirement for clinical adoption is that a model's predicted probabilities correspond to real-world risk. We evaluated this through calibration and borderline case analysis.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/reliability_diagram.png}}
\caption{Reliability Diagram and Probability Histogram. The distribution confirms that the model provides nuanced confidence scores, avoiding the "over-confidence" typical of unregularized networks.}
\label{fig:reliability}
\end{figure}

Figure \ref{fig:reliability} presents the probability distribution of the model's predictions. The high density near zero aligns with the dataset's prevalence, while the smooth decay toward higher probabilities indicates that the model is well-calibrated, providing a trustworthy spectrum of risk rather than binary guesses.

Furthermore, we investigated the model's robustness on "borderline" patients—individuals whose features lie near critical medical decision boundaries.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/borderline_bmi.png}}
\caption{Probability distribution for borderline BMI cases [24, 26].}
\label{fig:borderline_bmi}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/borderline_age.png}}
\caption{Probability distribution for high-risk age categories [8, 10].}
\label{fig:borderline_age}
\end{figure}

Figures \ref{fig:borderline_bmi} and \ref{fig:borderline_age} visualize the model's response to patients in the "grey area" of BMI (overweight threshold) and high-risk age groups (late 50s). In both scenarios, the model correctly shifts its confidence distribution compared to the general population, acknowledging the increased risk while maintaining the ability to distinguish between classes within these difficult subgroups. This behavior confirms that the system adheres to medical intuition in marginal cases.

\subsection{Evolutionary Algorithms Comparison}
 To further explore the landscape of bio-inspired optimization, we extended our study to include Differential Evolution (DE), Particle Swarm Optimization (PSO), and the Enhanced ALSHADE algorithm. Figure \ref{fig:evo_comparison} compares their convergence trajectories against the standard Genetic Algorithm.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/evo_comparison.png}}
\caption{Convergence of Evolutionary Algorithms: Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization (PSO), and Enhanced ALSHADE.}
\label{fig:evo_comparison}
\end{figure}

\textbf{Analysis:} All three evolutionary methods demonstrated capability in finding robust hyperparameters. PSO showed rapid early convergence (ROC-AUC 0.8184), efficiently exploiting local gradients in the hyperparameter space. Differential Evolution (DE) matched this performance (0.8173) with a smaller population size, indicating high sample efficiency. While the standard GA explored a broader diversity of solutions, DE and PSO offered a competitive alternative with potentially faster convergence for this specific topology of the loss landscape.

\subsection{Model Interpretation (XAI)}
 To understand \textit{what} the neural network learned, we performed Permutation Feature Importance analysis (Figure \ref{fig:feature_importance}).

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/feature_importance.png}}
\caption{Feature Importance (Permutation-based). Features like General Health (GenHlth), BMI, and Age are identified as the most critical predictors.}
\label{fig:feature_importance}
\end{figure}

\textbf{Analysis:} The model heavily weights \texttt{GenHlth} (Self-reported General Health), \texttt{BMI}, and \texttt{Age}. This is a positive finding for trustworthiness, as these features align perfectly with known medical risk factors for Diabetes. The model is not relying on spurious correlations but on medically valid signals.

\subsection{Neuro-Symbolic Validation and Error Analysis}
 A critical component of our study was analyzing where the model fails. We isolated the "Top 10 Worst Errors" (high-confidence False Negatives) and passed them through our Interpretability Layer. Table VI summarizes a representative subset.

\begin{table}[htbp]
\caption{Neuro-Symbolic Analysis of Hard False Negatives}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{ID} & \textbf{True} & \textbf{MLP Pred} & \textbf{Rule} & \textbf{Fuzzy} & \textbf{Bayes} \\
\hline
46974 & 1 & 0.0005 & Low & 0.0476 & 0.0000 \\
5789 & 1 & 0.0013 & Low & 0.0435 & 0.0000 \\
33533 & 1 & 0.0017 & Low & 0.0324 & 0.0001 \\
\hline
\end{tabular}
\label{tab:hard_examples}
\end{center}
\end{table}

\textbf{Insight:} In these "hard" cases, the patient effectively has Diabetes (True=1), but the MLP predicts Healthy with confidence near 100\%. Crucially, the Symbolic Layer (Rules, Fuzzy, Bayes) \textit{also} predicts Low Risk. This indicates these patients are statistical outliers—likely atypical presentations (e.g., young, low BMI diabetics) that do not follow standard patterns. While the symbolic layer didn't "correct" the MLP here, the \textit{agreement} between Neural and Symbolic systems confirms that the error stems from data limitation rather than model failure, valuable insight for a clinician.

Beyond these individual outliers, we performed a global quantitative analysis of the Neuro-Symbolic layer as a safety mechanism. Table \ref{tab:auditing} provides the concrete auditing counts.

\input{generated_tables/table_auditing.tex}

\begin{itemize}
    \item \textbf{Safety Net for Missed Diagnoses:} Even with a highly optimized neural baseline, the MLP produced 2,819 False Negatives. The Hybrid system successfully recovered a significant portion of these, reducing the FN count to 2,245. This represents a \textbf{20.36\% reduction in missed diagnoses}. This confirms the critical value of hybrid AI: explicit guidelines can catch risks that a statistical model might overlook due to data noise.
    \item \textbf{Trade-off:} As shown in Table \ref{tab:auditing}, this improvement in sensitivity comes with an increase in False Positives (from 7,085 to 10,353). In a diagnostic setting, this trade-off is often acceptable, as the cost of a missed diagnosis (False Negative) far outweighs the cost of an additional screening test (False Positive).
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{figures/confusion_matrix_hybrid.png}}
\caption{Confusion Matrix: Full Hybrid Neuro-Symbolic System. Compared to the standalone MLP, the hybrid system demonstrates improved sensitivity to the minority class.}
\label{fig:cm_hybrid}
\end{figure}

\input{generated_tables/table_nn_vs_hybrid_stat.tex}

\subsection{Ablation Study: The Hybrid Advantage}
 To validate the hybrid module, we removed components systematically. The results below indicate that the Fuzzy-Bayesian layer provides a crucial boost in metrics compared to the baseline MLP.



\begin{figure}[htbp]
\centerline{\includegraphics[width=0.8\linewidth]{figures/ablation_heatmap.png}}
\caption{Ablation Study Heatmap. The intensity represents the performance gain contributed by each module.}
\label{fig:ablation_heatmap}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.8\linewidth]{figures/ablation_pyramid.png}}
\caption{Ablation Pyramid. Visualizing the cumulative contribution of each component to the final model performance.}
\label{fig:ablation_pyramid}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/roc_curves_comparison.png}}
\caption{ROC Curve Comparison for the improved pipeline. The Optimized MLP (PCA + Lion) achieves peak discrimination (AUC = 0.821), while the Full Hybrid System integrates symbolic reasoning with a minor trade-off in aggregate discrimination (AUC = 0.793).}
\label{fig:roc_comparison}
\end{figure}



\section{Discussion}
 The integration of HPO and Neuro-Symbolic methods transforms a standard classification task into a robust, safety-critical diagnostic pipeline. Our results highlight a fundamental tension in medical AI: the trade-off between statistical discrimination and clinical safety.

\begin{itemize}
    \item \textbf{The Safety--Performance Trade-off:} The pure MLP achieved a higher ROC-AUC (0.82) compared to the Hybrid system (0.79). However, AUC is an aggregate metric that treats all errors equally. In a diagnostic setting, a False Negative (missing a sick patient) is catastrophic. Our Hybrid system's rule-based layer successfully recovered 20.36\% of the cases that the MLP missed. The drop in AUC reflects the system's "conservative" nature—it refuses to classify high-risk profiles as healthy even if the statistical signal is weak, effectively flattening the ROC curve but securing the decision boundary for critical cases.
    \item \textbf{Winning Configuration:} Among the extensive combinations, the \textbf{PCA representation paired with the Lion optimizer and Bayesian Optimization} emerged as the most clinically relevant baseline when employing the "Total Defense" strategy. This configuration achieved a peak \textbf{ROC-AUC of 0.8212}, a robust \textbf{F1-score of 0.4619}, and a \textbf{Recall of 0.6012}. The success of PCA suggests that dimensionality reduction effectively mitigated noise in the self-reported survey data, while the Lion optimizer's sign-based momentum demonstrated superior convergence properties on this non-convex tabular landscape compared to traditional Adaptive Moment estimation.
    \item \textbf{Addressing Imbalanced Data and Outliers:} Our path to obtaining these robust results involved a systematic evolution of our data and model pipeline. Initially, standard techniques like Z-score outlier removal aggressively pruned the minority diabetic class, exacerbating the 1:7 imbalance. We addressed this by implementing the **Isolation Forest** algorithm \cite{cheng2019iforest}, which isolates anomalies based on feature splits rather than statistical tails, preserving valid but rare clinical presentations. Furthermore, we integrated a "Total Defense" strategy comprising **Random Oversampling** to balance class counts during training, **Weighted Loss** (\texttt{BCEWithLogitsLoss}) to penalize false negatives heavily, and **Dynamic Threshold Moving** to optimize the decision boundary for F1-score rather than using a static 0.5 cutoff. This multi-layered approach was critical in boosting our Sensitivity and F1-scores significantly compared to our earlier, unweighted baselines, ensuring the model learned to prioritize disease detection.
    \item \textbf{Robustness via Disagreement:} The low global disagreement rate (2.64\%) indicates that for the vast majority of patients, the data-driven model aligns with expert medical heuristics. The cases where they \textit{do} disagree are not failures but "auditable moments." These are precisely the patients who deviate from the population norm and require human review. A pure black-box model would silently classify them with high confidence; our system flags them through this conflict.
    \item \textbf{Calibration and Trust:} While the MLP provides a probability, the Hybrid system provides a \textit{justification}. By fusing fuzzy risk scores and explicit rules, the system moves from "prediction" to "diagnosis," offering a transparent logic trail that a clinician can verify.
\end{itemize}

\section{Conclusion}
 Our journey began with a question: Can we make a high-performance neural network safe enough for medicine? We started by rigorously optimizing a deep learning baseline, achieving a peak \textbf{ROC-AUC of 0.8212} and a balanced \textbf{F1-score of 0.4619} using PCA features and the Lion optimizer. Yet, statistical power alone is not trustworthiness.

 When we exposed this "black box" to a Neuro-Symbolic safety layer, we uncovered a critical reality. While the hybrid integration lowered the aggregate discrimination score to 0.79, it achieved something far more valuable: it caught over 20\% of the false negatives that the neural network had silently missed. This result redefines success for medical AI. It is not about being right 99\% of the time on average; it is about \textit{never} being wrong when it matters most.

 We conclude that the future of medical diagnostics is not in replacing doctors with algorithms, but in arming them with systems like ours—systems that are smart enough to learn from data, but wise enough to follow the rules. This is not just a classifier; it is a safety net, and in healthcare, that difference saves lives.

\section*{Appendix: Reproducibility Details}
 To ensure the reproducibility of our findings, we provide the detailed parameters for the symbolic and optimization layers.

\subsection{Symbolic Rule Set}
 The expert layer utilizes five primary clinical rules derived from standard diabetes risk factors:
\begin{enumerate}
    \item \textbf{Rule 1 (Obesity):} IF (BMI $\ge$ 30) $\land$ (Age $\ge$ 45) $\rightarrow$ High Risk
    \item \textbf{Rule 2 (Comorbidities):} IF (HighBP == 1) $\land$ (HighChol == 1) $\rightarrow$ High Risk
    \item \textbf{Rule 3 (Sedentary):} IF (PhysActivity == 0) $\land$ (Fruits == 0) $\rightarrow$ High Risk
    \item \textbf{Rule 4 (Frailty):} IF (GenHlth $\ge$ 4) $\land$ (DiffWalk == 1) $\rightarrow$ High Risk
    \item \textbf{Rule 5 (Healthy):} IF (BMI $<$ 25) $\land$ (PhysActivity == 1) $\land$ (GenHlth $\le$ 2) $\rightarrow$ Low Risk
\end{enumerate}

\subsection{Fuzzy Membership Functions}
 The fuzzy inference system employs triangular membership functions defined by the triplet $[a, b, c]$ (start, peak, end):
\begin{itemize}
    \item \textbf{BMI (kg/m$^2$):} Low [10, 18.5, 24.9], Medium [20, 27.5, 30.0], High [25, 32.5, 60.0].
    \item \textbf{Age (Categories):} Young [1, 2.5, 5], Middle-Aged [3, 7, 11], Old [8, 11, 13].
\end{itemize}

\subsection{Hyperparameter Optimization Budgets}
 For all optimization experiments, the following computational budgets were enforced:
\begin{itemize}
    \item \textbf{Random Search \& Optuna:} 15 trials per configuration.
    \item \textbf{GA, DE, PSO:} Population size of 8 evolved over 4 generations (total 32 evaluations).
    \item \textbf{Enhanced ALSHADE:} Population size 8, 4 generations, with adaptive $F$ and $CR$.
\end{itemize}
Experiments were conducted using Python 3.13 and PyTorch 2.5 on a Linux-based workstation with a fixed random seed of 42.

\clearpage % Ensure bibliography starts on a new page
\setlength{\IEEElabelsep}{0.5em} % Reduce space between label and entry
\let\oldthebibliography\thebibliography
\let\endoldthebibliography\endthebibliography
\renewenvironment{thebibliography}[1]{%
  \begin{oldthebibliography}{#1}%
  \small % Use a slightly smaller font for the bibliography
  \setlength{\itemsep}{0pt}%      % Reduce space between items
  \setlength{\parskip}{0pt}%      % Reduce space between paragraphs
  \raggedbottom % Allow vertical space to stretch
}{%
  \end{oldthebibliography}%
}

\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{references}

\section*{Supplementary Material}

\input{generated_tables/table_ablation_full_stat.tex}
\input{generated_tables/table_opt_raw.tex}
\input{generated_tables/table_opt_pca.tex}
\input{generated_tables/table_opt_ae.tex}
\input{generated_tables/iso_tables/table_opt_raw_iso.tex}
\input{generated_tables/iso_tables/table_opt_pca_iso.tex}
\input{generated_tables/iso_tables/table_opt_ae_iso.tex}

\end{document}
