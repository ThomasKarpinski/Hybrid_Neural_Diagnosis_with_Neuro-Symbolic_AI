\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Trustworthy Hybrid Neural Diagnosis with Hyperparameter Optimization and Neuro-Symbolic Interpretability}

\author{\IEEEauthorblockN{1\textsuperscript{st} Tomasz Karpiński}
\IEEEauthorblockA{\textit{Faculty of Physics and Applied Computer Science} \\
\textit{AGH University of Krakow}\\
Krakow, Poland \\
tkarpinski@student.agh.edu.pl}
}

\maketitle

\begin{abstract}
The adoption of Artificial Intelligence in high-stakes medical domains is hindered by the "black box" nature of deep neural networks. While accurate, standard models often lack the interpretability required for clinical trust and robustness against edge cases. This paper addresses this gap by proposing a Hybrid Neuro-Symbolic System for trustworthy medical diagnosis. We combine a fixed-architecture Multi-Layer Perceptron (MLP), optimized via rigorous Hyperparameter Optimization (HPO) techniques including Random Search, Bayesian Optimization (Optuna), and Evolutionary Algorithms, with a symbolic expert layer comprising Rule-Based Reasoning, Fuzzy Logic, and Bayesian probabilistic updates. Using the CDC Diabetes Dataset as a proxy for medical diagnostics, our system achieves a robust ROC-AUC of approximately 0.82. Critically, we demonstrate that the symbolic layer provides essential interpretability and acts as a safety audit for neural predictions, identifying complex "false negative" cases that purely data-driven models miss. This work illustrates a pathway toward "Human-in-the-loop" AI systems that enhance medical expertise with transparent and calibrated decision support.
\end{abstract}

\begin{IEEEkeywords}
Neuro-Symbolic AI, Hyperparameter Optimization, Explainable AI, Medical Diagnosis, Trustworthy AI
\end{IEEEkeywords}

\section{Introduction}
In the domain of healthcare, the cost of an error is not measured in latency or throughput, but in human well-being. Chronic conditions like Diabetes require early and accurate detection to prevent severe complications. While Deep Learning has revolutionized predictive capabilities in many fields, its application in medicine faces a critical "Trust Gap." A neural network may predict a 99\% probability of health, but without an explanation, a clinician cannot verify the reasoning against medical guidelines. Furthermore, purely data-driven models are often "brittle," failing silently on out-of-distribution samples or rare edge cases.

To bridge this gap, we must move beyond black-box accuracy and towards **Trustworthy AI**. This requires systems that are not only high-performing but also interpretable, robust, and aligned with expert knowledge.

This paper presents a comprehensive implementation of a **Hybrid Neuro-Symbolic Diagnostic System**. We address the conflict between accuracy and interpretability by fusing two distinct paradigms: the statistical learning power of deep neural networks and the transparent, logic-driven reasoning of classical Artificial Intelligence.

Our contributions are three-fold:
\begin{enumerate}
    \item We implement and rigorously optimize a baseline MLP using three distinct Hyperparameter Optimization (HPO) strategies: Random Search, Bayesian Optimization (Optuna), and Genetic Algorithms, achieving an optimized ROC-AUC of 0.82.
    \item We augment this neural baseline with a "Neuro-Symbolic Interpretability Layer" consisting of explicit medical rules, fuzzy inference systems, and Bayesian probability updates, providing human-readable explanations for model predictions.
    \item We perform a thorough error analysis and unsupervised exploration (PCA, K-Means), demonstrating how hybrid systems can expose the limitations of pure Deep Learning on complex, hard-to-diagnose patient profiles.
\end{enumerate}

The remainder of this paper is organized as follows: Section II details our methodology, including dataset preprocessing, model architecture, and the neuro-symbolic components. Section III presents our experimental results on HPO convergence and interpretability. Section IV discusses the implications for robustness and clinical adoption, and Section V concludes with future directions.

\section{Methodology}

\subsection{Dataset}
This study utilizes the \textbf{CDC Diabetes Health Indicators Dataset}, a large-scale health survey widely used as a benchmark for predictive medical modeling. The dataset serves as a proxy for complex diagnostic tasks, containing features such as Body Mass Index (BMI), Age, High Blood Pressure, Cholesterol levels, and General Health self-assessments.

Figure \ref{fig:class_dist} illustrates the significant class imbalance in the dataset, with the non-diabetic class (0) vastly outnumbering the diabetic class (1). This imbalance necessitates the use of stratified sampling and metrics like ROC-AUC over simple accuracy.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{experiments/class_distribution.png}}
\caption{Class Distribution of the CDC Diabetes Dataset. The dataset is heavily imbalanced, posing a challenge for standard classification algorithms.}
\label{fig:class_dist}
\end{figure}

Preprocessing steps included:
\begin{itemize}
    \item \textbf{Data Cleaning:} Removal of statistical outliers to ensure robust training. Figures \ref{fig:outliers_before} and \ref{fig:outliers_after} demonstrate the effect of outlier removal on the feature distributions.
    \item \textbf{Standardization:} Features were scaled to zero mean and unit variance ($z$-score normalization) to facilitate stable gradient descent for the neural network.
    \item \textbf{Split:} The data was stratified into Training, Validation, and Test sets to ensure class distribution remained consistent.
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{experiments/outliers_before.png}}
\caption{Boxplots of feature distributions before outlier removal. Significant outliers are visible in BMI and PhysHlth.}
\label{fig:outliers_before}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{experiments/outliers_after.png}}
\caption{Boxplots of feature distributions after outlier removal. The distributions are more compact, reducing the risk of model instability.}
\label{fig:outliers_after}
\end{figure}

Figure \ref{fig:pairplot} provides a pairwise visualization of selected features, highlighting the complex, non-linear relationships and overlap between classes that motivate the use of a neural network over simpler linear models.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{experiments/pairplot.png}}
\caption{Pairplot of selected features colored by target class. The substantial overlap between classes indicates a non-trivial classification boundary.}
\label{fig:pairplot}
\end{figure}

\subsection{The Baseline Neural Network}
We implemented a fixed-architecture Multi-Layer Perceptron (MLP) designed for binary classification (Diabetes vs. Non-Diabetes). The architecture consists of:
\begin{itemize}
    \item \textbf{Input Layer:} 21 features (after preprocessing).
    \item \textbf{Hidden Layers:} Two dense layers with 32 and 16 neurons respectively, using ReLU activation functions to introduce non-linearity.
    \item \textbf{Output Layer:} A single neuron with a Sigmoid activation function to output a probability score $P(y=1|x) \in [0, 1]$.
    \item \textbf{Optimization:} The model is trained using the Adam optimizer and Binary Cross-Entropy loss.
\end{itemize}

\subsection{Hyperparameter Optimization (HPO)}
To ensure the neural network reached its maximum potential, we moved beyond manual tuning and employed three automated HPO strategies. The search space included Learning Rate ($10^{-5}$ to $10^{-2}$), Batch Size (32, 64, 128), Weight Decay ($L_2$ regularization), Dropout rate, and Adam $\beta$ parameters.

\subsubsection{Random Search}
Used as a stochastic baseline, Random Search samples hyperparameters uniformly from the defined space. It is simple to implement but inefficient in high-dimensional spaces.

\subsubsection{Bayesian Optimization (Optuna)}
We utilized Optuna with the Tree-structured Parzen Estimator (TPE) sampler. Unlike Random Search, Bayesian Optimization builds a probabilistic model of the objective function, allowing it to "focus" on promising regions of the hyperparameter space, theoretically converging faster to the global optimum.

\subsubsection{Evolutionary Algorithms (GA, DE, PSO)}
A population-based approach inspired by natural selection. We implemented a Genetic Algorithm where a population of hyperparameter configurations evolves over generations via crossover and mutation operations, optimizing for the ROC-AUC metric on the validation set.

\subsection{Neuro-Symbolic Interpretability Layer}
To address the "black box" nature of the MLP, we developed an expert layer that operates in parallel to the neural network.

\subsubsection{Rule-Based Reasoning}
We encoded explicit medical heuristics into IF-THEN rules. For example:
\begin{equation}
    \text{IF } (BMI \ge 30) \land (Age \ge 45) \rightarrow \text{High Risk}
\end{equation}
These rules act as "sanity checks," ensuring that high-risk profiles identified by established medical guidelines are not missed by the model.

\subsubsection{Fuzzy Logic Inference}
Medical thresholds are rarely binary. A BMI of 29.9 is medically similar to 30.1. We modeled continuous variables like Age and BMI using Fuzzy Sets (Low, Medium, High) with triangular membership functions. A "Fuzzy Risk Score" was computed by aggregating these memberships, providing a nuanced risk assessment $Score \in [0, 1]$.

\subsubsection{Bayesian Probability Update}
We implemented a Gaussian Naive Bayes classifier to compute the posterior probability of disease $P(Disease|Features)$ independently of the MLP. This provides a statistically grounded "second opinion" based purely on feature distributions.

\subsection{Unsupervised Analysis}
To understand the underlying structure of the patient data, we applied Unsupervised Learning techniques:
\begin{itemize}
    \item \textbf{PCA (Principal Component Analysis):} Reduced the 21-dimensional feature space to 2 principal components for visualization.
    \item \textbf{K-Means Clustering:} Grouped patients into latent clusters ($k=2$) to analyze if natural data groupings align with diagnostic labels.
\end{itemize}

\section{Experiments \& Results}

\subsection{HPO Performance and Convergence}
We evaluated the efficacy of the three HPO methods. Figure \ref{fig:hpo_convergence} illustrates the convergence of the best-found ROC-AUC score over sequential evaluation trials.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{experiments/hpo_convergence.png}}
\caption{HPO Convergence: Comparison of Random Search, Bayesian Optimization (Optuna), and Genetic Algorithms. Optuna demonstrates rapid convergence to the optimal region.}
\label{fig:hpo_convergence}
\end{figure}

Furthermore, to gain deeper insights into the hyperparameter search landscape, we visualize the distributions of evaluated hyperparameter combinations against their resulting ROC-AUC scores. Figure \ref{fig:hpo_distributions} presents pairwise scatter plots of key continuous hyperparameters.

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{experiments/hpo_distributions.png}}
\caption{Hyperparameter Distributions vs. ROC-AUC. Pairwise scatter plots of Learning Rate, Weight Decay, Dropout, Beta1, and Beta2, colored by the achieved ROC-AUC, reveal the impact of hyperparameter choices on model performance.}
\label{fig:hpo_distributions}
\end{figure}


\textbf{Analysis:} As hypothesized, Bayesian Optimization (Optuna) proved the most efficient, rapidly identifying a high-performing configuration (ROC-AUC $\approx 0.820$) within fewer trials than Random Search. The Genetic Algorithm also performed well but required a larger number of evaluations to stabilize. The optimized MLP achieved a final Test ROC-AUC of \textbf{0.8204}, significantly robust for this complexity of data.

\begin{table}[htbp]
\caption{Comparison of Hyperparameter Optimization Methods}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Best ROC-AUC} & \textbf{Time (s)} & \textbf{Best Epochs} \\
\hline
Random Search & 0.8208 & 166.4 & 38 \\
Bayesian (Optuna) & \textbf{0.8205} & 602.0 & 66 \\
Genetic Algorithm & 0.8172 & 257.6 & 42 \\
\hline
\end{tabular}
\label{tab:hpo_comparison}
\end{center}
\end{table}

\subsection{Baseline Performance Metrics}
Before delving into HPO and advanced interpretability, we assess the baseline MLP's performance on the test set. Figure \ref{fig:confusion_matrix} presents the confusion matrix, which provides a detailed breakdown of correct and incorrect classifications.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{experiments/confusion_matrix.png}}
\caption{Confusion Matrix for the Baseline MLP on the Test Set. Values represent counts of true positives, true negatives, false positives, and false negatives.}
\label{fig:confusion_matrix}
\end{figure}

The matrix reveals a high number of True Negatives (correctly identified non-diabetic cases) but also a significant number of False Negatives (diabetic cases incorrectly classified as non-diabetic), reflecting the challenge of detecting the minority class and the imbalanced nature of the dataset.

\subsection{Evolutionary Algorithms Comparison}
To further explore the landscape of bio-inspired optimization, we extended our study to include Differential Evolution (DE) and Particle Swarm Optimization (PSO). Figure \ref{fig:evo_comparison} compares their convergence trajectories against the standard Genetic Algorithm.


\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{experiments/evo_comparison.png}}
\caption{Convergence of Evolutionary Algorithms: Genetic Algorithm (GA), Differential Evolution (DE), and Particle Swarm Optimization (PSO).}
\label{fig:evo_comparison}
\end{figure}

\textbf{Analysis:} All three evolutionary methods demonstrated capability in finding robust hyperparameters. PSO showed rapid early convergence (ROC-AUC 0.8184), efficiently exploiting local gradients in the hyperparameter space. Differential Evolution (DE) matched this performance (0.8173) with a smaller population size, indicating high sample efficiency. While the standard GA explored a broader diversity of solutions, DE and PSO offered a competitive alternative with potentially faster convergence for this specific topology of the loss landscape.

\subsection{Model Interpretation (XAI)}
To understand \textit{what} the neural network learned, we performed Permutation Feature Importance analysis (Figure \ref{fig:feature_importance}).

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{experiments/feature_importance.png}}
\caption{Feature Importance (Permutation-based). Features like General Health (GenHlth), BMI, and Age are identified as the most critical predictors.}
\label{fig:feature_importance}
\end{figure}

\textbf{Analysis:} The model heavily weights \texttt{GenHlth} (Self-reported General Health), \texttt{BMI}, and \texttt{Age}. This is a positive finding for trustworthiness, as these features align perfectly with known medical risk factors for Diabetes. The model is not relying on spurious correlations but on medically valid signals.

\subsection{Neuro-Symbolic Validation and Error Analysis}
A critical component of our study was analyzing where the model fails. We isolated the "Top 10 Worst Errors" (high-confidence False Negatives) and passed them through our Interpretability Layer. Table I summarizes a representative subset.

\begin{table}[htbp]
\caption{Neuro-Symbolic Analysis of Hard False Negatives}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{ID} & \textbf{True} & \textbf{MLP Pred} & \textbf{Rule} & \textbf{Fuzzy} & \textbf{Bayes} \\
\hline
46974 & 1 & 0.0015 & Low & 0.0476 & 0.0000 \\
5487 & 1 & 0.0020 & None & 0.1786 & 0.0001 \\
34660 & 1 & 0.0023 & Low & 0.0952 & 0.0003 \\
\hline
\end{tabular}
\label{tab:hard_examples}
\end{center}
\end{table}

\textbf{Insight:} In these "hard" cases, the patient effectively has Diabetes (True=1), but the MLP predicts Healthy with $>99\%$ confidence. Crucially, the Symbolic Layer (Rules, Fuzzy, Bayes) \textit{also} predicts Low Risk. This indicates these patients are statistical outliers—likely atypical presentations (e.g., young, low BMI diabetics) that do not follow standard patterns. While the symbolic layer didn't "correct" the MLP here, the \textit{agreement} between Neural and Symbolic systems confirms that the error stems from data limitation rather than model failure, valuable insight for a clinician.

Beyond these individual outliers, we performed a global quantitative analysis of the Neuro-Symbolic layer as a safety mechanism.
\begin{itemize}
    \item \textbf{Safety Net for Missed Diagnoses:} When the MLP produced a False Negative (missing a diabetic case), the Rule-Based system successfully flagged the patient as "High Risk" in \textbf{80.02\%} of cases. This confirms the critical value of hybrid AI: explicit guidelines can catch obvious risks that a statistical model might overlook due to data noise.
    \item \textbf{Correction of False Alarms:} Conversely, when the MLP produced a False Positive (incorrectly predicting diabetes), the Fuzzy Inference system correctly indicated "Low Risk" in \textbf{99.37\%} of cases.
    \item \textbf{Alignment:} The MLP and Fuzzy Logic system showed high global alignment, disagreeing on only 2.64\% of the entire test set, reinforcing the model's adherence to valid medical features like BMI and Age.
\end{itemize}

\subsection{Unsupervised Insights}
Figures \ref{fig:pca_labels} and \ref{fig:pca_clusters} visualize the data structure via PCA.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{experiments/unsupervised_pca_labels.png}}
\caption{PCA Visualization colored by True Labels (Diabetes vs. No Diabetes).}
\label{fig:pca_labels}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{experiments/unsupervised_pca_clusters.png}}
\caption{PCA Visualization colored by K-Means Clusters.}
\label{fig:pca_clusters}
\end{figure}

\textbf{Analysis:} The PCA plots reveal significant overlap between classes, confirming the inherent difficulty of the classification task. K-Means clustering ($k=2$) captures the broad structural split but does not perfectly align with the disease labels, suggesting that "Diabetes" is not a single, geometrically distinct cluster in feature space but rather a complex boundary condition.

\section{Discussion}
The integration of HPO and Neuro-Symbolic methods transforms a standard classification task into a robust diagnostic pipeline.
\begin{itemize}
    \item \textbf{Robustness:} The alignment of Feature Importance with medical intuition suggests the model is robust to noise in irrelevant features.
    \item \textbf{Calibration:} The high ROC-AUC combined with the probabilistic outputs of the Bayesian layer allows for calibrated risk scoring.
    \item \textbf{Safety Net:} While the rules did not catch every False Negative, they provide a mechanism for "Knowledge Injection." If a new medical guideline emerges (e.g., a new high-risk age group), it can be instantly added to the Rule Layer without retraining the neural network.
\end{itemize}

\section{Conclusion}
We have successfully developed and validated a Hybrid Neuro-Symbolic System for medical diagnosis. By combining the optimized performance of an MLP (via Bayesian Optimization) with the transparency of Rule-Based and Fuzzy Logic, we achieved a high-performing model (ROC-AUC 0.82) that remains interpretable.

This work demonstrates that "Trustworthy AI" is not about sacrificing accuracy for explainability, but about integrating them. In a real-world clinical setting, such a system would operate as a "Second Opinion" tool—flagging high-risk patients for review and providing the \textit{reasons} (Rules/Features) for its decision, ultimately empowering doctors to make better, data-informed decisions.

Future work will focus on "Gating Networks" to automatically switch between Neural and Symbolic inference modes based on prediction uncertainty, further closing the loop between human expertise and machine learning.

\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
