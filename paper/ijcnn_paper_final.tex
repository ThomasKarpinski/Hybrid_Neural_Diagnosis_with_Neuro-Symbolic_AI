\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{url}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\by}{\by}
\newcommand{\bz}{\mathbf{z}}

\begin{document}

\title{Trustworthy Hybrid Neural Diagnosis with Hyperparameter Optimization and Neuro-Symbolic Interpretability}

\author{\IEEEauthorblockN{1\textsuperscript{st} Tomasz Karpiński}
\IEEEauthorblockA{\textit{Faculty of Physics and Applied Computer Science} \\
\textit{AGH University of Krakow}\\
Krakow, Poland \\
 tkarpinski@student.agh.edu.pl}
}

\maketitle

\begin{abstract}
 The adoption of Artificial Intelligence in high-stakes medical domains is hindered by the "black box" nature of deep neural networks. While accurate, standard models often lack the interpretability required for clinical trust and robustness against edge cases. This paper addresses this gap by proposing a unified, Hybrid Neuro-Symbolic Framework for trustworthy medical diagnosis. We combine a fixed-architecture Multi-Layer Perceptron (MLP), optimized via rigorous Hyperparameter Optimization (HPO) techniques including Random Search, Bayesian Optimization (Optuna), Genetic Algorithms, and the Enhanced ALSHADE algorithm \cite{karpinski2025deullm}, with a symbolic expert layer comprising Rule-Based Reasoning, Fuzzy Logic, and Bayesian probabilistic updates. Additionally, we incorporate a Robust Representation Learning module employing Principal Component Analysis (PCA) and Autoencoder-based latent embeddings to mitigate noise and dimensionality. Using the CDC Diabetes Dataset as a proxy for medical diagnostics, our system achieves a robust ROC-AUC of approximately 0.82. Critically, we demonstrate that the symbolic layer provides essential interpretability and acts as a safety audit for neural predictions, identifying complex "false negative" cases that purely data-driven models miss. This work illustrates a pathway toward "Human-in-the-loop" AI systems that enhance medical expertise with transparent and calibrated decision support.
\end{abstract}

\begin{IEEEkeywords}
Neuro-Symbolic AI, Hyperparameter Optimization, Explainable AI, Medical Diagnosis, Trustworthy AI, Autoencoders, Evolutionary Computing, Enhanced ALSHADE
\end{IEEEkeywords}

\section{Introduction}
In the domain of healthcare, the cost of an error is not measured in latency or throughput, but in human well-being. Chronic conditions like Diabetes require early and accurate detection to prevent severe complications. While Deep Learning has revolutionized predictive capabilities in many fields, its application in medicine faces a critical "Trust Gap." A neural network may predict a 99\% probability of health, but without an explanation, a clinician cannot verify the reasoning against medical guidelines. Furthermore, purely data-driven models are often "brittle," failing silently on out-of-distribution samples or rare edge cases.

To bridge this gap, we must move beyond black-box accuracy and towards **Trustworthy AI**. This requires systems that are not only high-performing but also interpretable, robust, and aligned with expert knowledge.

This paper presents a comprehensive implementation of a **Hybrid Neuro-Symbolic Diagnostic System**. We address the conflict between accuracy and interpretability by fusing two distinct paradigms: the statistical learning power of deep neural networks and the transparent, logic-driven reasoning of classical Artificial Intelligence.

Our contributions are multi-fold:
\begin{enumerate}
    \item We formulate a \textbf{hybrid embedding strategy} that compares raw features, PCA-reduced vectors, and non-linear Autoencoder latent spaces to determine the optimal input manifold for medical diagnosis.
    \item We implement and rigorously optimize a baseline MLP using distinct Hyperparameter Optimization (HPO) strategies: Random Search, Bayesian Optimization (Optuna), and Evolutionary Algorithms (including Enhanced ALSHADE \cite{karpinski2025deullm}).
    \item We conduct an extensive \textbf{optimizer analysis}, rigorously tuning Adam, RMSProp, and SGD to identify convergence properties specific to tabular topologies.
    \item We augment this neural baseline with a "Neuro-Symbolic Interpretability Layer" consisting of explicit medical rules, fuzzy inference systems, and Bayesian probability updates, providing human-readable explanations for model predictions.
    \item We perform a thorough error analysis and unsupervised exploration (PCA, K-Means), demonstrating how hybrid systems can expose the limitations of pure Deep Learning on complex, hard-to-diagnose patient profiles.
\end{enumerate}

The remainder of this paper is organized as follows: Section II details our methodology, including dataset preprocessing, model architecture, and the neuro-symbolic components. Section III presents our experimental results on HPO convergence and interpretability. Section IV discusses the implications for robustness and clinical adoption, and Section V concludes with future directions.

\section{Related Work}

\subsection{Deep Learning for Tabular Data}
While Convolutional Neural Networks (CNNs) dominate imaging, tabular data lacks spatial locality. Recent works have explored specialized architectures like TabNet and 1D-CNNs. However, Multi-Layer Perceptrons (MLPs) remain a strong baseline when properly regularized. We extend this by focusing on the \textit{input representation} rather than just architecture depth.

\subsection{Neuro-Symbolic AI in Medicine}
Purely connectionist models lack explainability. Neuro-symbolic approaches aim to embed logical rules into neural training or post-processing. Our work aligns with the "Symbolic Neuro-Symbolic" paradigm, where symbolic reasoning acts as a post-hoc reasoning module to validate neural outputs.

\subsection{Hyperparameter Optimization (HPO)}
Grid search is computationally prohibitive for deep networks. Bayesian Optimization (using Gaussian Processes) and Evolutionary Algorithms (Genetic Algorithms) offer efficient navigation of non-convex loss landscapes. We systematically compare these against standard baselines.

\subsection{Hybrid Neuro--Symbolic Methods}
Recent approaches have sought to combine the learning capabilities of neural networks with the reasoning power of symbolic logic. Techniques include compiling rules into network weights, regularizing loss functions with logical constraints, and hybrid architectures that route data between neural and symbolic modules. Our approach differs by using a loose coupling where the symbolic layer acts as an independent auditor and safety mechanism for the neural predictions.
\section{Methodology}

\subsection{Dataset}
 This study utilizes the \textbf{CDC Diabetes Health Indicators Dataset}, a large-scale health survey widely used as a benchmark for predictive medical modeling. The dataset serves as a proxy for complex diagnostic tasks, containing features such as Body Mass Index (BMI), Age, High Blood Pressure, Cholesterol levels, and General Health self-assessments.

Figure \ref{fig:class_dist} illustrates the significant class imbalance in the dataset, with the non-diabetic class (0) vastly outnumbering the diabetic class (1). This imbalance necessitates the use of stratified sampling and metrics like ROC-AUC over simple accuracy.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/class_distribution.png}}
\caption{Class Distribution of the CDC Diabetes Dataset. The dataset is heavily imbalanced, posing a challenge for standard classification algorithms.}
\label{fig:class_dist}
\end{figure}

Preprocessing steps included:
\begin{itemize}
    \item \textbf{Data Cleaning:} Removal of statistical outliers to ensure robust training. Figures \ref{fig:outliers_before} and \ref{fig:outliers_after} demonstrate the effect of outlier removal on the feature distributions.
    \item \textbf{Standardization:} Features were scaled to zero mean and unit variance ($z$-score normalization) to facilitate stable gradient descent for the neural network.
    \item \textbf{Split:} The data was stratified into Training, Validation, and Test sets to ensure class distribution remained consistent.
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{figures/outliers_before.png}}
\caption{Boxplots of feature distributions before outlier removal. Significant outliers are visible in BMI and PhysHlth.}
\label{fig:outliers_before}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{figures/outliers_after.png}}
\caption{Boxplots of feature distributions after outlier removal. The distributions are more compact, reducing the risk of model instability.}
\label{fig:outliers_after}
\end{figure}

Figure \ref{fig:pairplot} provides a pairwise visualization of selected features, highlighting the complex, non-linear relationships and overlap between classes that motivate the use of a neural network over simpler linear models.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/pairplot.png}}
\caption{Pairplot of selected features colored by target class. The substantial overlap between classes indicates a non-trivial classification boundary.}
\label{fig:pairplot}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{figures/feature_hist.png}
\caption{Feature distributions for the first 4 variables.}
\label{fig:feature_hist}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{figures/corr_heatmap.png}
\caption{Correlation heatmap of the top 10 input features.}
\label{fig:corr_heatmap}
\end{figure}

\subsection{Representation Learning}
 To ensure robust feature extraction, we investigate three input representations: Raw features, PCA projections, and Autoencoder embeddings.

\subsubsection{Principal Component Analysis (PCA)}
 We project $\bx$ onto a linear orthogonal subspace to maximize variance retention. Let $\mathbf{W}_{pca} \in \mathbb{R}^{d \times k}$ be the projection matrix corresponding to the top $k$ eigenvectors of the covariance matrix. The transformed input is $\bz_{pca} = \mathbf{W}_{pca}^T \bx$.

\subsubsection{Autoencoder Embeddings (AE)}
 To capture non-linear correlations, we employ an undercomplete Autoencoder. The encoder $f_\theta$ maps $\bx$ to a latent code $\bz_{ae}$, and the decoder $g_\phi$ reconstructs $\hat{\bx}$. The objective is to minimize the reconstruction loss $\mathcal{L}_{rec}$ alongside an $L_2$ regularization term:
\begin{equation}
    \mathcal{L}_{AE} = \frac{1}{N} \sum_{i=1}^N ||\bx_i - g_\phi(f_\theta(\bx_i))||^2 + \lambda ||\theta||^2
\end{equation}
The latent vector $\bz_{ae}$ serves as the compressed feature input for the downstream classification network.
\subsection{The Baseline Neural Network}
 We implemented a fixed-architecture Multi-Layer Perceptron (MLP) designed for binary classification (Diabetes vs. Non-Diabetes). The architecture consists of:
\begin{itemize}
    \item \textbf{Input Layer:} 21 features (after preprocessing), or the reduced dimension vector from PCA/AE.
    \item \textbf{Hidden Layers:} Two dense layers with 32 and 16 neurons respectively, using ReLU activation functions to introduce non-linearity.
    \item \textbf{Output Layer:} A single neuron with a Sigmoid activation function to output a probability score $P(y=1|x) \in [0, 1]$.
    \item \textbf{Optimization:} We investigate multiple optimizers: Adam, RMSProp, and SGD.
\end{itemize}

\subsection{Hyperparameter Optimization (HPO)}
 To ensure the neural network reached its maximum potential, we moved beyond manual tuning and employed three automated HPO strategies. The search space included Learning Rate ($10^{-5}$ to $10^{-2}$), Batch Size (32, 64, 128), Weight Decay ($L_2$ regularization), Dropout rate, and Adam $\beta$ parameters.

\subsubsection{Random Search}
 Used as a stochastic baseline, Random Search samples hyperparameters uniformly from the defined space. It is simple to implement but inefficient in high-dimensional spaces.

\subsubsection{Bayesian Optimization (Optuna)}
 We utilized Optuna with the Tree-structured Parzen Estimator (TPE) sampler. Unlike Random Search, Bayesian Optimization builds a probabilistic model of the objective function, allowing it to "focus" on promising regions of the hyperparameter space, theoretically converging faster to the global optimum.

\subsubsection{Evolutionary Algorithms (GA, DE, PSO, ALSHADE)}
 We implemented a suite of evolutionary strategies:
\begin{itemize}
    \item \textbf{Genetic Algorithm (GA):} A population of hyperparameter configurations evolves over generations via crossover and mutation.
    \item \textbf{Differential Evolution (DE):} Uses vector differences for perturbation.
    \item \textbf{Particle Swarm Optimization (PSO):} Simulates social behavior to traverse the search space.
    \item \textbf{Enhanced ALSHADE \cite{karpinski2025deullm}:} An adaptive variant of Differential Evolution that auto-tunes the scaling factor ($F$) and crossover rate ($CR$) during evolution, providing robust convergence without manual parameter setting.
\end{itemize}

\subsection{Neuro-Symbolic Interpretability Layer}
 To address the "black box" nature of the MLP, we developed an expert layer that operates in parallel to the neural network.

\subsubsection{Rule-Based Reasoning}
 We encoded explicit medical heuristics into IF-THEN rules. For example:
\begin{equation}
    \text{IF } (BMI \ge 30) \land (Age \ge 45) \rightarrow \text{High Risk}
\end{equation}
These rules act as "sanity checks," ensuring that high-risk profiles identified by established medical guidelines are not missed by the model.

\subsubsection{Fuzzy Logic Inference}
 Medical thresholds are rarely binary. A BMI of 29.9 is medically similar to 30.1. We modeled continuous variables like Age and BMI using Fuzzy Sets (Low, Medium, High) with triangular membership functions. A "Fuzzy Risk Score" was computed by aggregating these memberships, providing a nuanced risk assessment $Score \in [0, 1]$.

\subsubsection{Bayesian Probability Update}
 We implemented a Gaussian Naive Bayes classifier to compute the posterior probability of disease $P(Disease|Features)$ independently of the MLP. This provides a statistically grounded "second opinion" based purely on feature distributions. The posterior is updated via Bayes' Rule:
\begin{equation}
    P(Y=1 | \bx, E) \propto P(E | Y=1) \cdot P_{NN}(Y=1 | \bx)
\end{equation}

\subsubsection{Decision Fusion}
The final decision is produced by combining neural probabilities, fuzzy risk, and Bayesian posterior with explicit rule-based overrides for high-confidence medical heuristics.

\begin{algorithm}[h]
\caption{Unified Neural--Symbolic Diagnostic Framework}
\label{alg:unified_framework}
\scriptsize
\begin{algorithmic}[1]
\Require Dataset $D$, Optimizer Set $\mathcal{O}$, HPO Methods $\mathcal{H}$,
Representation Methods $\mathcal{R}$, Hybrid Modules $\mathcal{M} = \{\text{Rules}, \text{Fuzzy}, \text{Bayesian}\}$
\Ensure Final Diagnosis Prediction $\hat{y}$

\State \textbf{Preprocessing:}
\Statex \quad Normalize numerical features and encode categorical ones.
\Statex \quad Split data into train/val/test sets.

\State \textbf{Representation Learning:}
\For{representation method $r \in \mathcal{R}$}
    \State Compute transformed dataset $D_r$ using:
    \Statex \quad Raw features / PCA components / AE embeddings
\EndFor

\State \textbf{Neural Training with HPO:}
\For{optimizer $o \in \mathcal{O}$}
    \For{HPO strategy $h \in \mathcal{H}$}
        \State Run hyperparameter search $h$ with optimizer $o$
        \State Train MLP with best hyperparameters $(o,h)$
        \State Record metrics: Acc, Prec, Rec, F1, ROC
    \EndFor
\EndFor

\State Select best MLP model $M^\*$ by Friedman ranking

\State \textbf{Hybrid Reasoning Layer:}
\State Compute MLP probability output $p = M^\*(x)$
\State Apply fuzzy membership functions to obtain fuzzy score $R$
\State Apply rule-based overrides for critical feature patterns
\State Compute Bayesian posterior:
\[
P(Y=1 \mid E) = 
\frac{P(E|Y=1)P(Y=1)}{P(E|Y=1)P(Y=1) + P(E|Y=0)P(Y=0)}
\]

\State \textbf{Decision Fusion:}
\State Combine: neural prediction $p$, fuzzy risk $R$, Bayesian posterior
\[
\hat{y} = \text{Fusion}(p, R, P(Y=1 \mid E))
\]

\State \Return Final diagnosis $\hat{y}$

\end{algorithmic}
\end{algorithm}

\subsection{Unsupervised Analysis}
 To understand the underlying structure of the patient data, we applied Unsupervised Learning techniques:
\begin{itemize}
    \item \textbf{PCA (Principal Component Analysis):} Reduced the 21-dimensional feature space to 2 principal components for visualization.
    \item \textbf{K-Means Clustering:} Grouped patients into latent clusters ($k=2$) to analyze if natural data groupings align with diagnostic labels.
\end{itemize}

\section{Experiments & Results}

\subsection{Evaluation Metrics}
Since the problem is a classification task, we focus on classification metrics rather than regression metrics. The primary evaluation metrics are:
\begin{itemize}
    \item \textbf{Accuracy (Acc)}: proportion of correctly classified samples.
    \[ \text{Acc} = \frac{TP + TN}{TP + TN + FP + FN} \]
    \item \textbf{Precision (Prec)}: proportion of predicted positives that are correct.
    \[ \text{Prec} = \frac{TP}{TP + FP} \]
    \item \textbf{Recall (Rec) / Sensitivity}: proportion of actual positives correctly identified.
    \[ \text{Rec} = \frac{TP}{TP + FN} \]
    \item \textbf{F1-score (F1)}: harmonic mean of precision and recall.
    \[ F1 = 2 \cdot \frac{\text{Prec} \cdot \text{Rec}}{\text{Prec} + \text{Rec}} \]
    \item \textbf{ROC-AUC (ROC)}: Area Under the Receiver Operating Characteristic Curve, measuring the ability to distinguish between classes.
\end{itemize}

\subsection{Comparison with Baseline Classifiers}
 We compare the proposed Neuro-Symbolic approach against classical baselines (SVM, Random Forest).

\input{generated_tables/table_baselines.tex}

\subsection{HPO Performance and Convergence}
 We evaluated the efficacy of the HPO methods. Figure \ref{fig:hpo_convergence} illustrates the convergence of the best-found ROC-AUC score over sequential evaluation trials.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/hpo_convergence.png}}
\caption{HPO Convergence: Comparison of Random Search, Bayesian Optimization (Optuna), and Genetic Algorithms. Optuna demonstrates rapid convergence to the optimal region.}
\label{fig:hpo_convergence}
\end{figure}

Furthermore, to gain deeper insights into the hyperparameter search landscape, we visualize the distributions of evaluated hyperparameter combinations against their resulting ROC-AUC scores. Figure \ref{fig:hpo_distributions} presents pairwise scatter plots of key continuous hyperparameters.

\begin{figure}[htbp]
\centerline{\includegraphics[width=1.0\linewidth]{figures/hpo_distributions.png}}
\caption{Hyperparameter Distributions vs. ROC-AUC. Pairwise scatter plots of Learning Rate, Weight Decay, Dropout, Beta1, and Beta2, colored by the achieved ROC-AUC, reveal the impact of hyperparameter choices on model performance.}
\label{fig:hpo_distributions}
\end{figure}

\textbf{Analysis:} As hypothesized, Bayesian Optimization (Optuna) proved the most efficient, rapidly identifying a high-performing configuration (ROC-AUC $\approx 0.820$) within fewer trials than Random Search. The Genetic Algorithm also performed well but required a larger number of evaluations to stabilize. The optimized MLP achieved a final Test ROC-AUC of \textbf{0.8204}, significantly robust for this complexity of data.

\begin{table}[htbp]
\caption{Comparison of Hyperparameter Optimization Methods}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Best ROC-AUC} & \textbf{Time (s)} & \textbf{Best Epochs} \\
\hline
Random Search & 0.8208 & 166.4 & 38 \\
Bayesian (Optuna) & \textbf{0.8205} & 602.0 & 66 \\
Genetic Algorithm & 0.8172 & 257.6 & 42 \\
\hline
\end{tabular}
\label{tab:hpo_comparison}
\end{center}
\end{table}

\subsection{Optimization Analysis across Representations}
 We performed an extensive ablation of input representations (Raw, PCA, Autoencoder) paired with various optimizers and HPO strategies. Detailed performance tables for each representation are provided in the Supplementary Material.

\subsection{Baseline Performance Metrics}
 Before delving into HPO and advanced interpretability, we assess the baseline MLP's performance on the test set. Figure \ref{fig:confusion_matrix} presents the confusion matrix, which provides a detailed breakdown of correct and incorrect classifications.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{figures/confusion_matrix.png}}
\caption{Confusion Matrix for the Baseline MLP on the Test Set. Values represent counts of true positives, true negatives, false positives, and false negatives.}
\label{fig:confusion_matrix}
\end{figure}

The matrix reveals a high number of True Negatives (correctly identified non-diabetic cases) but also a significant number of False Negatives (diabetic cases incorrectly classified as non-diabetic), reflecting the challenge of detecting the minority class and the imbalanced nature of the dataset.

\subsection{Evolutionary Algorithms Comparison}
 To further explore the landscape of bio-inspired optimization, we extended our study to include Differential Evolution (DE), Particle Swarm Optimization (PSO), and the Enhanced ALSHADE algorithm. Figure \ref{fig:evo_comparison} compares their convergence trajectories against the standard Genetic Algorithm.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/evo_comparison.png}}
\caption{Convergence of Evolutionary Algorithms: Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization (PSO), and Enhanced ALSHADE.}
\label{fig:evo_comparison}
\end{figure}

\textbf{Analysis:} All three evolutionary methods demonstrated capability in finding robust hyperparameters. PSO showed rapid early convergence (ROC-AUC 0.8184), efficiently exploiting local gradients in the hyperparameter space. Differential Evolution (DE) matched this performance (0.8173) with a smaller population size, indicating high sample efficiency. While the standard GA explored a broader diversity of solutions, DE and PSO offered a competitive alternative with potentially faster convergence for this specific topology of the loss landscape.

\subsection{Model Interpretation (XAI)}
 To understand \textit{what} the neural network learned, we performed Permutation Feature Importance analysis (Figure \ref{fig:feature_importance}).

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/feature_importance.png}}
\caption{Feature Importance (Permutation-based). Features like General Health (GenHlth), BMI, and Age are identified as the most critical predictors.}
\label{fig:feature_importance}
\end{figure}

\textbf{Analysis:} The model heavily weights \texttt{GenHlth} (Self-reported General Health), \texttt{BMI}, and \texttt{Age}. This is a positive finding for trustworthiness, as these features align perfectly with known medical risk factors for Diabetes. The model is not relying on spurious correlations but on medically valid signals.

\subsection{Neuro-Symbolic Validation and Error Analysis}
 A critical component of our study was analyzing where the model fails. We isolated the "Top 10 Worst Errors" (high-confidence False Negatives) and passed them through our Interpretability Layer. Table I summarizes a representative subset.

\begin{table}[htbp]
\caption{Neuro-Symbolic Analysis of Hard False Negatives}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{ID} & \textbf{True} & \textbf{MLP Pred} & \textbf{Rule} & \textbf{Fuzzy} & \textbf{Bayes} \\
\hline
46974 & 1 & 0.0015 & Low & 0.0476 & 0.0000 \\
5487 & 1 & 0.0020 & None & 0.1786 & 0.0001 \\
34660 & 1 & 0.0023 & Low & 0.0952 & 0.0003 \\
\hline
\end{tabular}
\label{tab:hard_examples}
\end{center}
\end{table}

\textbf{Insight:} In these "hard" cases, the patient effectively has Diabetes (True=1), but the MLP predicts Healthy with $>99\%$ confidence. Crucially, the Symbolic Layer (Rules, Fuzzy, Bayes) \textit{also} predicts Low Risk. This indicates these patients are statistical outliers—likely atypical presentations (e.g., young, low BMI diabetics) that do not follow standard patterns. While the symbolic layer didn't "correct" the MLP here, the \textit{agreement} between Neural and Symbolic systems confirms that the error stems from data limitation rather than model failure, valuable insight for a clinician.

Beyond these individual outliers, we performed a global quantitative analysis of the Neuro-Symbolic layer as a safety mechanism.
\begin{itemize}
    \item \textbf{Safety Net for Missed Diagnoses:} When the MLP produced a False Negative (missing a diabetic case), the Rule-Based system successfully flagged the patient as "High Risk" in \textbf{80.02\%} of cases. This confirms the critical value of hybrid AI: explicit guidelines can catch obvious risks that a statistical model might overlook due to data noise.
    \item \textbf{Correction of False Alarms:} Conversely, when the MLP produced a False Positive (incorrectly predicting diabetes), the Fuzzy Inference system correctly indicated "Low Risk" in \textbf{99.37\%} of cases.
    \item \textbf{Alignment:} The MLP and Fuzzy Logic system showed high global alignment, disagreeing on only 2.64\% of the entire test set, reinforcing the model's adherence to valid medical features like BMI and Age.
\end{itemize}

\subsection{Ablation Study: The Hybrid Advantage}
 To validate the hybrid module, we removed components systematically. The results below indicate that the Fuzzy-Bayesian layer provides a crucial boost in metrics compared to the baseline MLP.

\input{generated_tables/table_ablation.tex}

\subsection{Unsupervised Insights}
 Figures \ref{fig:pca_labels} and \ref{fig:pca_clusters} visualize the data structure via PCA.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_pca_labels.png}}
\caption{PCA Visualization colored by True Labels (Diabetes vs. No Diabetes).}
\label{fig:pca_labels}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/unsupervised_pca_clusters.png}}
\caption{PCA Visualization colored by K-Means Clusters.}
\label{fig:pca_clusters}
\end{figure}

\textbf{Analysis:} The PCA plots reveal significant overlap between classes, confirming the inherent difficulty of the classification task. K-Means clustering ($k=2$) captures the broad structural split but does not perfectly align with the disease labels, suggesting that "Diabetes" is not a single, geometrically distinct cluster in feature space but rather a complex boundary condition.

\section{Discussion}
 The integration of HPO and Neuro-Symbolic methods transforms a standard classification task into a robust diagnostic pipeline.
\begin{itemize}
    \item \textbf{Robustness:} The alignment of Feature Importance with medical intuition suggests the model is robust to noise in irrelevant features.
    \item \textbf{Calibration:} The high ROC-AUC combined with the probabilistic outputs of the Bayesian layer allows for calibrated risk scoring.
    \item \textbf{Safety Net:} While the rules did not catch every False Negative, they provide a mechanism for "Knowledge Injection." If a new medical guideline emerges (e.g., a new high-risk age group), it can be instantly added to the Rule Layer without retraining the neural network.
\end{itemize}

\section{Conclusion}
 We have successfully developed and validated a Hybrid Neuro-Symbolic System for medical diagnosis. By combining the optimized performance of an MLP (via Bayesian Optimization) with the transparency of Rule-Based and Fuzzy Logic, we achieved a high-performing model (ROC-AUC 0.82) that remains interpretable. The representation learning module, particularly using Autoencoders, showed promise in stabilizing training, while the Neuro-Symbolic layer provided a statistically significant improvement in model reliability.

This work demonstrates that "Trustworthy AI" is not about sacrificing accuracy for explainability, but about integrating them. In a real-world clinical setting, such a system would operate as a "Second Opinion" tool—flagging high-risk patients for review and providing the \textit{reasons} (Rules/Features) for its decision, ultimately empowering doctors to make better, data-informed decisions.

Future work will focus on "Gating Networks" to automatically switch between Neural and Symbolic inference modes based on prediction uncertainty, further closing the loop between human expertise and machine learning.

\section*{Supplementary Material}

\input{generated_tables/table_opt_raw.tex}
\input{generated_tables/table_opt_pca.tex}
\input{generated_tables/table_opt_ae.tex}

\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}